import puppeteer from "puppeteer";
import fs from "fs/promises";
import path from "path";
import { crawlerLogger } from "../utils/logger";
// import { saveJobDetailToWeaviate } from "../api/weaviate/saveJobDetailToWeaviate";
import { JobDetail } from "../types/job";
import {
  saveJobDetailToWeaviate,
  updateJobDetailInWeaviate,
} from "../api/weaviate/saveJobDetailToWeaviate";
import axios from "axios";

const DATA_DIR = path.join(__dirname, "../../data");

interface WeaviateResponse {
  data?: {
    Get?: {
      JobDetails?: { detailedText: string | null }[];
    };
  };
}

// âœ… ì±„ìš© ìƒì„¸ ì •ë³´ ìŠ¤í¬ë˜í•‘ í•¨ìˆ˜
const jobKoreaDetailScrape = async (jobId: string, jobUrl: string) => {
  crawlerLogger.info(`âœ… [JobKorea Detail Scraper] ì‹¤í–‰: ${jobUrl}`);

  // âœ… ì €ì¥í•  JSON íŒŒì¼ ê²½ë¡œ ì„¤ì •
  const now = new Date();
  const formattedDate = now.toISOString().slice(0, 10).replace(/-/g, ""); // YYYYMMDD
  const scheduledHour =
    now.getHours() >= 9 && now.getHours() < 15 ? "morning" : "afternoon";
  const detailsPath = path.join(
    DATA_DIR,
    formattedDate,
    scheduledHour,
    "details"
  );
  const filePath = path.join(detailsPath, `${jobId}.json`);

  try {
    // âœ… JSON íŒŒì¼ì´ ì¡´ì¬í•˜ëŠ” ê²½ìš° ê·¸ëŒ€ë¡œ ë°˜í™˜
    await fs.access(filePath);
    crawlerLogger.info(`ğŸ“‚ [JobKorea Detail] ê¸°ì¡´ ë°ì´í„° ì¡´ì¬: ${filePath}`);
    const existingData = await fs.readFile(filePath, "utf-8");
    return JSON.parse(existingData);
  } catch {
    crawlerLogger.info(
      `âš ï¸ [JobKorea Detail] ê¸°ì¡´ ë°ì´í„° ì—†ìŒ â†’ í¬ë¡¤ë§ ì‹¤í–‰! (${filePath})`
    );
  }

  try {
    const browser = await puppeteer.launch({ headless: true });
    const page = await browser.newPage();
    await page.goto(jobUrl, { waitUntil: "networkidle2" });

    const jobDetail = await page.evaluate(() => {
      // âœ… íŠ¹ì • ìš”ì†Œì˜ í…ìŠ¤íŠ¸ ê°€ì ¸ì˜¤ëŠ” í•¨ìˆ˜
      const getText = (selector: string): string => {
        const element = document.querySelector(selector);
        return element?.textContent?.trim().replace(/\s+/g, " ") ?? "";
      };

      // âœ… "dt" íƒœê·¸ë¥¼ ì°¾ì•„ì„œ ë¼ë²¨ì´ íŠ¹ì • ê°’ì¸ ê²½ìš° í•´ë‹¹ "dd" íƒœê·¸ ê°’ ê°€ì ¸ì˜¤ê¸°
      const getTextByLabel = (label: string, excludeLabel?: string): string => {
        const dtElements = Array.from(document.querySelectorAll("dt"));
        for (const dt of dtElements) {
          const text = dt.textContent?.trim();
          if (
            text?.includes(label) &&
            (!excludeLabel || !text.includes(excludeLabel))
          ) {
            const dd = dt.nextElementSibling;
            return dd?.textContent?.trim().replace(/\s+/g, " ") ?? "-";
          }
        }
        return "-";
      };

      const getJobTitle = (): string => {
        const element = document.querySelector(".sumTit h3.hd_3");
        if (!element) return "";

        return Array.from(element.childNodes)
          .filter((node) => node.nodeType === Node.TEXT_NODE) // í…ìŠ¤íŠ¸ ë…¸ë“œë§Œ ì¶”ì¶œ
          .map((node) => node.textContent?.trim())
          .join(" ");
      };

      const isImageType = !!document.querySelector("td.detailTable img");
      const detailedTextElement = document.querySelector(
        "td.detailTable .content_sec"
      ) as HTMLElement;
      const detailedText =
        detailedTextElement?.innerHTML?.trim() || "EMPTY_HTML";

      return {
        title: getJobTitle(), // âœ… ì§ë¬´ëª…
        company: getText(".coName"), // âœ… íšŒì‚¬ëª…
        experience: getTextByLabel("ê²½ë ¥"), // âœ… ê²½ë ¥
        education: getTextByLabel("í•™ë ¥"), // âœ… í•™ë ¥
        employmentType: getTextByLabel("ê³ ìš©í˜•íƒœ"), // âœ… ê³ ìš©í˜•íƒœ
        salary: getTextByLabel("ê¸‰ì—¬"), // âœ… ê¸‰ì—¬
        location: getTextByLabel("ì§€ì—­"), // âœ… ê·¼ë¬´ ì§€ì—­
        workingHours: getTextByLabel("ê·¼ë¬´ì‹œê°„", "ë‚¨ì€ì‹œê°„"), // âœ… ê·¼ë¬´ ì‹œê°„ (ë‚¨ì€ì‹œê°„ ì œì™¸)
        skills: getTextByLabel("ìŠ¤í‚¬"), // âœ… ê¸°ìˆ  ìŠ¤íƒ
        industry: getTextByLabel("ì‚°ì—…(ì—…ì¢…)"), // âœ… ì‚°ì—…
        employees: getTextByLabel("ì‚¬ì›ìˆ˜"), // âœ… ì‚¬ì› ìˆ˜
        established: getTextByLabel("ì„¤ë¦½ë…„ë„"), // âœ… ì„¤ë¦½ë…„ë„
        companyType: getTextByLabel("ê¸°ì—…í˜•íƒœ"), // âœ… ê¸°ì—… í˜•íƒœ
        website: getTextByLabel("í™ˆí˜ì´ì§€"), // âœ… í™ˆí˜ì´ì§€
        coreCompetencies: getTextByLabel("í•µì‹¬ì—­ëŸ‰"), // âœ… í•µì‹¬ì—­ëŸ‰ (ì˜ì–´ ë³€í™˜)
        preferredQualifications: getTextByLabel("ìš°ëŒ€"), // âœ… ìš°ëŒ€ì‚¬í•­ (ì˜ì–´ ë³€í™˜)
        position: getTextByLabel("ì§ì±…"), // âœ… ì§ì±… (ì˜ì–´ ë³€í™˜)
        certification: getTextByLabel("ì¸ì¦"), // âœ… ì¸ì¦ (ì˜ì–´ ë³€í™˜)
        revenue: getTextByLabel("ë§¤ì¶œì•¡"), // âœ… ë§¤ì¶œì•¡ (ì˜ì–´ ë³€í™˜)
        isImageType,
        detailedText,
      };
    });

    await saveJobDetailToWeaviate(jobId, jobDetail);

    if (!jobDetail.isImageType) {
      crawlerLogger.info(
        `ğŸŸ¢ [DEBUG] ì´ë¯¸ì§€ê°€ ì•„ë‹ˆë¯€ë¡œ ì¶”ê°€ ìŠ¤í¬ë˜í•‘ ì§„í–‰: ${jobId}`
      );
      const updatedJobDetail = await scrapeJobDetailedInfo(
        jobId,
        jobUrl,
        jobDetail
      );
      return updatedJobDetail;
    }

    await browser.close();
    return jobDetail;
  } catch (error) {
    crawlerLogger.error("âŒ [JobKorea Detail Scraper] ì˜¤ë¥˜ ë°œìƒ:", error);
    return null;
  }
};

// âœ… ì¶”ê°€ ìŠ¤í¬ë˜í•‘ í•¨ìˆ˜ (í…ìŠ¤íŠ¸/í…Œì´ë¸” í˜•íƒœë§Œ)
const scrapeJobDetailedInfo = async (
  jobId: string,
  jobUrl: string,
  existingJobDetail: JobDetail
): Promise<JobDetail> => {
  crawlerLogger.info(
    `ğŸ” [JobKorea Additional Scraper] ìƒì„¸ ìš”ê°• ìŠ¤í¬ë˜í•‘: ${jobUrl}`
  );

  const browser = await puppeteer.launch({ headless: true });
  const page = await browser.newPage();
  await page.goto(jobUrl, { waitUntil: "networkidle2" });

  let detailedText = "";

  try {
    await page.waitForSelector('iframe[name="gib_frame"]', { timeout: 10000 });
    crawlerLogger.info(`ğŸŸ¢ [DEBUG] iFrame ìš”ì†Œ ê°ì§€ ì™„ë£Œ: ${jobId}`);

    const iframeElement = await page.$('iframe[name="gib_frame"]');
    const iframe = await iframeElement?.contentFrame();

    if (iframe) {
      await iframe.waitForSelector(".detailTable", { timeout: 10000 });

      detailedText = await iframe.evaluate(() => {
        const container = document.querySelector(".detailTable") as HTMLElement;
        if (!container) return "EMPTY_TEXT";

        const elements = container.querySelectorAll(
          "p, li, h1, h2, h3, span, strong"
        );

        return (
          Array.from(
            new Set(
              Array.from(elements)
                .map((el) => el.textContent?.trim() ?? "")
                .filter(Boolean)
            )
          ).join("\n") || "EMPTY_TEXT"
        );
      });

      crawlerLogger.info(
        `âœ… [JobKorea Additional Scraper] ìƒì„¸ ìš”ê°• ì¶”ì¶œ ì™„ë£Œ: ${jobId}`
      );
    } else {
      crawlerLogger.error(
        `âŒ [JobKorea Additional Scraper] iFrame ì½˜í…ì¸  ì ‘ê·¼ ì‹¤íŒ¨: ${jobId}`
      );
    }
  } catch (error) {
    crawlerLogger.error(
      `âŒ [JobKorea Additional Scraper] ìŠ¤í¬ë˜í•‘ ì˜¤ë¥˜ ë°œìƒ: ${error}`
    );
  } finally {
    await browser.close();
  }

  const updatedJobDetail: JobDetail = {
    ...existingJobDetail,
    detailedText,
  };

  // âœ… Weaviateì—ì„œ `detailedText` ì¡°íšŒ
  let existingDetailedText: string | null = null;

  try {
    const response = await axios.post<WeaviateResponse>(
      "http://localhost:8080/v1/graphql",
      {
        query: `{
                Get {
                    JobDetails(where: { path: ["jobId"], operator: Equal, valueString: "${jobId}" }) {
                        detailedText
                    }
                }
            }`,
      }
    );

    existingDetailedText =
      response.data?.data?.Get?.JobDetails?.[0]?.detailedText ?? null;
  } catch (error) {
    crawlerLogger.error(`âŒ [Weaviate] ê¸°ì¡´ ë°ì´í„° ì¡°íšŒ ì‹¤íŒ¨: ${jobId}`, error);
  }

  if (
    !existingDetailedText ||
    existingDetailedText === "EMPTY_TEXT" ||
    existingDetailedText.trim() === ""
  ) {
    await updateJobDetailInWeaviate(
      jobId,
      updatedJobDetail.detailedText ?? "EMPTY_TEXT"
    );
  } else {
    crawlerLogger.info(
      `ğŸ”„ [Weaviate] ê¸°ì¡´ detailedTextê°€ ì¡´ì¬í•˜ì—¬ ì—…ë°ì´íŠ¸ ìƒëµ: ${jobId}`
    );
  }

  // âœ… ê¸°ì¡´ detailedTextê°€ ì—†ê±°ë‚˜ "EMPTY_HTML"ì´ë©´ ì—…ë°ì´íŠ¸ ì§„í–‰
  if (
    !existingDetailedText ||
    existingDetailedText === "EMPTY_HTML" ||
    existingDetailedText.trim() === ""
  ) {
    await updateJobDetailInWeaviate(
      jobId,
      updatedJobDetail.detailedText ?? "EMPTY_TEXT"
    );
  } else {
    crawlerLogger.info(
      `ğŸ”„ [Weaviate] ê¸°ì¡´ detailedTextê°€ ì¡´ì¬í•˜ì—¬ ì—…ë°ì´íŠ¸ ìƒëµ: ${jobId}`
    );
  }

  crawlerLogger.info(
    `âœ… [JobKorea Additional Scraper] ìƒì„¸ ìš”ê°• ìµœì¢… ì—…ë°ì´íŠ¸ ì™„ë£Œ: ${jobId}`
  );

  return updatedJobDetail;
};

export { jobKoreaDetailScrape, scrapeJobDetailedInfo };
